"""
Evaluate the model, currently just with the dataset
"""
import os
import sys

import numpy as np
import onnxruntime as ort


if __name__ == '__main__':
    MODEL_PATH = "./model.onnx"
    ROOT_DIRECTORY = "validate"
    TENSOR_SIZE = 2048

    providers = ort.get_available_providers()
    inner_session = ort.InferenceSession(
        MODEL_PATH,
        providers=providers,
    )

    data_paths = []
    labels = []

    # Walk through the root directory to get all files and their labels
    for label_dir in ['benign', 'malicious']:
        # pylint: disable=invalid-name
        label = 0 if label_dir == 'benign' else 1
        dir_path = os.path.join(ROOT_DIRECTORY, label_dir)
        for file_name in os.listdir(dir_path):
            file_path = os.path.join(dir_path, file_name)
            if os.path.isfile(file_path) and os.path.getsize(file_path) > 1000:
                data_paths.append(file_path)
                labels.append(label)

    result_list = []
    # pylint: disable=invalid-name
    count = 1
    print()
    for data_path, label in zip(data_paths, labels):
        # Load the binary file
        raw_data = np.fromfile(data_path, dtype=np.int8).astype(np.float32)

        # Normalize the data
        data_max = np.max(raw_data)
        data_min = np.min(raw_data)
        raw_data = (raw_data - data_min) / (data_max - data_min)

        data_tensor = raw_data[:(TENSOR_SIZE ** 2 * 3)]

        # Padding (if necessary) to ensure all input sequences have the same length
        if len(data_tensor) < (TENSOR_SIZE ** 2 * 3):
            padding = np.zeros((TENSOR_SIZE ** 2 * 3) - len(data_tensor), dtype=np.float32)
            data_tensor = np.concatenate([data_tensor, padding])

        data_tensor = data_tensor.reshape((3, TENSOR_SIZE, TENSOR_SIZE))

        result = inner_session.run(
            None,
            {
                inner_session.get_inputs()[0]
                .name: np.expand_dims(data_tensor, 0)
            }
        )
        # pylint: disable=invalid-name
        if result[0][0][0] > 0.5:
            classification = 1
        else:
            classification = 0
        result_list.append(int(classification == label))
        sys.stdout.write("\033[F")
        print(f"[{count}/{len(data_paths)}]")
        count += 1
    print(result_list, f"\nData points: {len(result_list)}")
    print(f"Accuracy: {sum(result_list) / len(result_list) * 100}%")
