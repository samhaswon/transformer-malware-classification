"""
Train the model
"""

import os
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from model import create_model, get_device


class MalwareDataset(Dataset):
    """
    Custom Dataset class for loading malware/benign files
    """
    def __init__(self, root_dir: str, tensor_size: int, transform=None):
        """
        Args:
            root_dir (string): Directory with all the subdirectories (benign/malware).
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.tensor_size = tensor_size
        self.transform = transform
        self.data = []
        self.labels = []

        # Walk through the root directory to get all files and their labels
        for label_dir in ['finite', 'infinite']:
            label = 0 if label_dir == 'finite' else 1
            dir_path = os.path.join(root_dir, label_dir)
            for file_name in os.listdir(dir_path):
                file_path = os.path.join(dir_path, file_name)
                self.data.append(file_path)
                self.labels.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        file_path = self.data[idx]
        label = self.labels[idx]

        # Load the binary file
        raw_data = np.fromfile(file_path, dtype=np.int8).astype(np.float32)

        # Normalize the data
        data_max = np.max(raw_data)
        data_min = np.min(raw_data)
        raw_data = (raw_data - data_min) / (data_max - data_min)

        data_tensor = torch.tensor(raw_data, dtype=torch.float32)[:self.tensor_size]

        # Padding (if necessary) to ensure all input sequences have the same length
        if len(data_tensor) < self.tensor_size:
            padding = torch.zeros(self.tensor_size - len(data_tensor))
            data_tensor = torch.cat([data_tensor, padding])

        if self.transform:
            data_tensor = self.transform(data_tensor)

        return data_tensor, torch.tensor(label, dtype=torch.float32)


def save_model(model_in: nn.Module, device: str) -> None:
    """
    Saves the model in ONNX format.

    Parameters:
        model_in (nn.Module): The trained model.
        device (torch.device): The device where the model is located.
    """
    input_tensor_size = (1, 1, 2**13)
    x = torch.randn(*input_tensor_size, requires_grad=True)
    x = x.to(device)

    onnx_file_name = "model.onnx"
    torch.onnx.export(
        model_in,
        (x, x),
        onnx_file_name,
        export_params=True,
        opset_version=16,
        do_constant_folding=True,
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    )
    print("Model saved to:", onnx_file_name, "\n")


if __name__ == '__main__':
    ROOT_DIRECTORY = "features"
    NUM_EPOCHS = 100

    DEVICE = str(get_device())
    # Model instantiation
    model = create_model(d_size=2**13)
    model.to(DEVICE)

    dataset = MalwareDataset(ROOT_DIRECTORY, tensor_size=2**13)
    dataloader = DataLoader(dataset, batch_size=6, shuffle=True)

    # Loss and optimizer
    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Training loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        running_loss = 0.0

        for data, labels in dataloader:
            # Reshape the data for the transformer.
            # It expects (batch_size, sequence_length, d_model)
            data = data.unsqueeze(1)
            # For BCELoss, labels need to be of shape (batch_size, 1, 1)
            labels = labels.unsqueeze(1).unsqueeze(1)

            data = data.to(DEVICE)
            labels = labels.to(DEVICE)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(data, data)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {running_loss / len(dataloader)}")

    print("Finished Training")
    save_model(model, DEVICE)
