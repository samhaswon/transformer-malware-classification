"""
Train the model
"""

import os
import sys
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
# pylint: disable=import-error
from model import create_model, get_device


class MalwareDataset(Dataset):
    """
    Custom Dataset class for loading malware/benign files
    """
    def __init__(self, root_dir: str, tensor_size: int, transform=None):
        """
        Args:
            root_dir (string): Directory with all the subdirectories (benign/malware).
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.tensor_size = tensor_size
        self.transform = transform
        self.data = []
        self.labels = []

        # Walk through the root directory to get all files and their labels
        for label_dir in ['benign', 'malicious']:
            label = 0 if label_dir == 'benign' else 1
            dir_path = os.path.join(root_dir, label_dir)
            for file_name in os.listdir(dir_path):
                file_path = os.path.join(dir_path, file_name)
                if os.path.isfile(file_path) and os.path.getsize(file_path) > 1000:
                    self.data.append(file_path)
                    self.labels.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        file_path = self.data[idx]
        label = self.labels[idx]

        # Load the binary file
        raw_data = np.fromfile(file_path, dtype=np.int8).astype(np.float32)

        # Normalize the data
        data_max = np.max(raw_data)
        data_min = np.min(raw_data)
        raw_data = (raw_data - data_min) / (data_max - data_min)

        data_tensor = torch.tensor(raw_data[:(self.tensor_size ** 2 * 3)], dtype=torch.float32)

        # Padding (if necessary) to ensure all input sequences have the same length
        if len(data_tensor) < (self.tensor_size ** 2 * 3):
            padding = torch.zeros((self.tensor_size ** 2 * 3) - len(data_tensor))
            data_tensor = torch.cat([data_tensor, padding])

        data_tensor = data_tensor.reshape((3, self.tensor_size, self.tensor_size))

        if self.transform:
            data_tensor = self.transform(data_tensor)

        return data_tensor, torch.tensor(label, dtype=torch.float32)


def load_checkpoint(model_in, optimizer_in, scaler, file_path="checkpoint.pth") -> None:
    """
    Load a saved checkpoint.
    :param model_in: The model (variable) to load into.
    :param optimizer_in: The optimizer (variable) to load into.
    :param scaler: The model's scaler.
    :param file_path: The path to the checkpoint.
    :return: None
    """
    if os.path.isfile(file_path):
        checkpoint = torch.load(file_path)
        model_in.load_state_dict(checkpoint["state"]["state_dict"])
        optimizer_in.load_state_dict(checkpoint["state"]["optimizer"])
        scaler.load_state_dict(checkpoint["state"]["scaler"])
    else:
        print(f"Checkpoint not found at `{file_path}`")


def save_model(model_in: nn.Module, optimizer_in, scaler, device: str, t_size: int) -> None:
    """
    Saves the model in ONNX format.

    Parameters:
        model_in (nn.Module): The trained model.
        optimizer_in: The model's optimizer (for the checkpoint).
        scaler: The model's scaler.
        device (torch.device): The device where the model is located.
        t_size (int): The size of the feature portion of the tensor used.
    """
    input_tensor_size = (1, 3, t_size, t_size)
    x = torch.randn(*input_tensor_size, requires_grad=True)
    x = x.to(device)

    onnx_file_name = "model.onnx"
    torch.onnx.export(
        model_in,
        x,
        onnx_file_name,
        export_params=True,
        opset_version=16,
        do_constant_folding=True,
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    )
    print(f"Model saved to: {onnx_file_name}\t", end="")
    torch.save(
        {"state":
            {
                "state_dict": model_in.state_dict(),
                "optimizer": optimizer_in.state_dict(),
                "scaler": scaler.state_dict()
            }
        },
        "checkpoint.pth")
    print("Checkpoint saved to: checkpoint.pth\n")


if __name__ == '__main__':
    ROOT_DIRECTORY = "features/train"
    EVAL_DIRECTORY = "features/eval"
    NUM_EPOCHS = 100
    TENSOR_SIZE = 2048

    DEVICE = str(get_device())
    # Model instantiation
    print("Instantiating model")
    model = create_model(d_size=TENSOR_SIZE)

    # Loss and optimizer
    criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss for binary classification
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    model.to(DEVICE)

    dataset = MalwareDataset(ROOT_DIRECTORY, tensor_size=TENSOR_SIZE)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)

    eval_dataset = MalwareDataset(EVAL_DIRECTORY, tensor_size=TENSOR_SIZE)
    eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=True)

    scheduler = CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1E-6)
    grad_scaler = torch.cuda.amp.GradScaler()

    load_checkpoint(model, grad_scaler, optimizer)

    print("Starting training")

    # Training loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        # A variable, not a constant, so ignore the warning
        # pylint: disable=invalid-name
        running_loss = 0.0
        count = 1
        print(" ")
        for data, labels in dataloader:
            sys.stdout.write("\033[F")
            print(f"\tIteration {count}/{len(dataloader)} ", end="")
            count += 1
            # Reshape the data for the transformer.
            # It expects (batch_size, sequence_length, d_model)
            # data = data.unsqueeze(1)
            # For BCELoss, labels need to be of shape (batch_size, 1)
            labels = labels.unsqueeze(1)

            data = data.to(DEVICE)
            labels = labels.to(DEVICE)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Autocast for the forward pass in FP16 (NVIDIA Turing architecture and newer)
            with torch.autocast(device_type=str(DEVICE), dtype=torch.float16):
                # Forward pass
                outputs = model(data)
                loss = criterion(outputs, labels)

            # Backward pass and optimize
            grad_scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), max_norm=1.0
            )  # Clip gradients if their norm exceeds 1
            # loss.backward()
            grad_scaler.step(optimizer)
            grad_scaler.update()
            optimizer.zero_grad()
            scheduler.step()

            running_loss += loss.item()
            print(f"Loss: {loss.item()}")
        save_model(model, optimizer, grad_scaler, DEVICE, t_size=TENSOR_SIZE)

        # Do a model validation pass
        model.eval()
        result_list = []
        # pylint: disable=invalid-name
        count = 1
        for data, labels in eval_dataloader:
            sys.stdout.write("\033[F")
            print(f"\tEval iteration {count}/{len(eval_dataloader)}")
            labels = labels.unsqueeze(1)

            data = data.to(DEVICE)
            labels = labels.to(DEVICE)

            result = model(data)
            result_np = result.cpu().data.numpy()

            # pylint: disable=invalid-name
            if result > 0.5:
                classification = 1
            else:
                classification = 0
            result_list.append(int(classification == labels))
            count += 1

        print(f"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {running_loss / len(dataloader)}")
        print(f"Accuracy: {sum(result_list) / len(result_list) * 100:.4f}%\n")

    print("Finished Training. Saving model")
    save_model(model, optimizer, grad_scaler, DEVICE, t_size=TENSOR_SIZE)
